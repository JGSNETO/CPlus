#Class x Structure

-In structure the data members default to public and class they default to private. 

#Getters x Setters:
-Setters: Set value of data members
-Getters: Get value of data members

#namespace
Managing naming colisions

#Operators overload
-It is not unique to c++. 
-Two ways:
1. With members functions as part of a class definition. 
2. With non-members functions. 
Structure:

nameOfTheClass operator operator(itself) the arguments
Rational operator = (const Rational &);

#Allocate memory
-New and delete are used to create and delete objects from memory. 

#Lambda functions 
-Lambda functions or expressions : Is an anonymous function with the ability
to refer to identifiers outside of its own scope. 
-Good when you dont need to re-use the code. 

#Compiler structure:
1. Preprocessor 
2. Compiler
3. Optimizer 
4. Linker 

#Macros:
#define ONE 1
constexpr int ONE = 1 

#Conditional processing 

#ifdenf CONDITIONAL_H_
#define CONDITIONAL_H_

#define TWO 2

#endif /* CONDITIONAL_H_ */

#Abastract factory design pattern

-Provides na interface for creating families of related objects without specifying
Their concrete classes
-"Factory of factories"
-Each factory will create their own variation

#Builder

-Separetes construction of complex objects from its representation

-This allows us to build a complex object one step at a time

-Same construction process for different representations

E.g: Fast food combo meal

-The builder design pattern encapsulates the build

-Unlike others design pattern the concrete builder can return products that do not share

The same interface or base class

-Builders share the same steps to build a object

-The builder pattern can be useful in cases when complex, unrelated objects are needed

#Prototype design pattern

-Creates new objects cloning a existing one

-Is like cell division or cloning. Traits are like the properties or a object in programming

How is different from a copy constructor ?

-For a copy constructor we need to know exactly what object needs to be copied.

-Clone a existing object with prototype design its very simple, not only simple, allow us to copy the private fields that cant be acessed with a method externally.

#Shallow copy: Only the members of a object are copied over any reference objects are not copied

#Deep copy: Makes copies of reference objects
#Singleton

-Must be globally accessible

-Must only be one instance of a class

-What makes this pattern particularly is that its constructor is private. This means that only

The singleton itself is able to create a singleton

-Singleton pattern ensure a object is globally accessible and has one instance.

#Parallel and Concurrent Programming c++

Action: Install Cygwin

#Parallel Computing Hardware

Serial execution: One by one execution

Limited by the processor
Parallel execution

image.png
 

Parallel execution increases throughput
Accomplish a single task faster
Accomplish more task in a given time
Flynn's taxonomy
image.png
 

image.png
#SISD

-Sequential computer with a single processor

image.png
#SIMD

-Type o parallel computer with multiple processor units

-All the processors execute the same instruction in any given time, but they can

Each operate in different data element

image.png
#MISD

-Each unit executes a instruction, but in the same string of data

image.png
#MIMD

-Each unit executes a instruction with different data

 

#Parallel programming model

-Single program, Multiple data(spmd)

image.png
-Multiple program, multiple data(MPMD)

#Shared memory

-All processors acess the same memory with global address space

-If i change something in our shared memory space, everyother processor

Will be able to see it.

#Shared Memory Architectures

-Uniformed memory Access(UMA)

-Non-uniform memory access(NUMA)

 

image.png
 
 

#Threads and Processes
#Processes

-Include code, data, and state information

-Independent instance of a running program

-Separate address space
#Thread

-Independent path of execution

-Subset of a process

-Operating system schedules threads for execution

image.png
 

#Inter-process communication (IPC)

-Sockets and pipes

-Shared memory

-Remote procedure calls

 

#Threads vs Processes

-Threads are "lightweight" - require less overhead to create and terminate

-Operating system can switch between threads faster than processes


#Concurrency

-Ability of a program to be broken into parts that can run independently

Of each other.

#Concurency

-Program structure

-Dealing with multiple things at once

 

#Pallelism

-Simultaneous execution

-Doing multiple things at once

 

#Scheduler

-Operating system function that assigns processes  and threads to run on available

CPUs.

 

#Context Switch

-Storing the state of a process or thread to resume later

-Loading the saved state for the new process or thread to run

 

#Scheduling algorithms

-FIFO

-Shortest job next

-Priotrity

#Scheculing goals

-Maximize thoughput

-Maximize fairness

-Minimize wait time

-Minimize latency

 

 

 

#join()

 

-Wait until another thread completes its execution

#Garbage Collector

 

-Automatic memory management

-Reclaims memory no longer in use by program

 

#Daemon thread(Background)

-Does not prevent the process from terminating

-By default, threads are created as non-daemon

 

 

#Mutal exclusion

 

#Data race

-Problem that occurs when:

Two or more concurrent threads access the same memory location
At least one thread is modifying it
 

Read-Modify-Write

 

#Detecting Data Races

Hard to do
 

#Preventing data races

Pay attention whenever two or more threads access the same resources
 

#Mutual exclusion

#Critical section

-Code segment that accesses a shared resource

-Should not be executed by more than one thread or process at a time

 

#Mutex(lock)

-Mechanism to implement mutual exclusion

-Only one thread or process can possess at a time

 

#Atomic Operations

-Execute as a single action, relative to other threads

-Cannot be interrupted by other concurrent threads

 

#Acquiring a lock

-If lock is already taken, block/wait for it to be available

 

#Locks

 

#Deadlock

-All processes and threads are unable to continue executing

#Reentrant Mutex

-Can be locked multiple time by the same thread

-Must be unlocked as many time as it was locked

#Common Terms

-Reentrant Mutex

-Reentrant lock

-Recursive Mutex

-Recursive Lock

 

#Try lock

-Non-Blocking lock/Acquire method for mutex

-If the mutex is available, lock it and return TRUE

-If the mutex is not available, immediately retun FALSE

 

#Shared mutex

 

#Reader-Writer Lock

-Shared Read: Multiple threads at once

Threads Reading > Threads Writing

Or

-Exclusive write: Only one thread at a time

 

#DeadLock

-Each member is waiting for another member to take action


#Liveness

-Properties that require a system to make progress

-Members may have to "take turns" in critical sections

#Livelock

-Multiple threads or processes are activily responding to each other

To resolve conflict, but that prevents them from making progress

#Parallel and Concurrent Programming with C++ PT.2

#Synchronization

#Conditional Variable
-Queue of threads waiting for a certain condition 
-Associated with a mutex

#Monitor
-Protect section of code with mutual exclusion 
-Provide the ability for threads to wait until a condition occurs

#Three Operations:
1.Wait
-Automatically release lock on the mutex
-Go to sleep and enter waiting Queue
-Reacquire lock when woken up
2.Signal
3.Broadcast
 

 #Signal

-Wake up one thread from conditional variable queue

-Also called notify or wake

 

#Broadcast

-Wake upp all threads from conditional variable queue

-Also called notify all or wake all

 

#Shared Queue or Buffer

-Mutex

-Conditional variables

-BufferNotFull

 

#Procucer-Consumer Pattern

-Producer(s)

Add elements to shared data structure
-Consumer(s)

     -    Remove elements from shared data structure

 

FIFO

-Items are removed in the same order that they are added to the queue

-First item added will be first item removed

 

Synchronization Challanges

-Enforce mutual exclusion of producers and consumers

-Prevent producers from trying to add data to a full queue

-Prevent consumers from trying to remove data from a empty queue

 

#Unbounded Queue

-A queue with na unlimited capacity is still limited by meory

 

Average rate of production < Average rate of consumption

 

#Pipeline

-Chain of processing elements

 

#Semaphore

-Synchronization mechanism

-Can be used by multiple threads at the same time

-Includes a counter to track availability

 

#acquire()

-If counter is positive, decrement counter

-If counter is zero, wait until available

 

#release()

-Increment counter

-Signal waiting thread

 

#Mutex x Semaphore

-Mutex: Can only be acquired/released by the same thread

-Semaphore:Can be acquired/released by different threads

 

#Barriers

 

#Race condition

-Searching for race conditions: Use sleep statements to modify timing and execution order

-Heisenbug: A sw bug that disappears when you try to study it

 

#Barrier

-Prevents a group of threads from proceeding until enough threads have reached the barrier

#Thread Pool

 

-Creates and maintains a collection of worker threads

-Reuses existing threads to executes task

 

#Future

-Placeholder for a result that will be avaiable later

-Mechanism to access the result of a asynchronous operation

#Divide and Conquer

Divide a larg problem into subproblems
Conquer the subproblems by solving them recursively
Combine the solutions to the subproblems

#Evaluating Parallel Performance

#Weak scaling

-Variable number of processors with fixed problem size per processor

-Accomplish more work in the same time

#Strong scalling

-Variable number of processors with fixed size total problem size

-Accomplish same work in less time

#Tools that improve your code

 

-The compiler know the language better than you do

-Turn on all warnings, and dont ignore them

-Often a warning is a bug

 

#Static analyzers

 

-Tool that automatically examines C++ code for issues

-Analysis is much slower than compilation

-Dozens of static analyzers, open source and commercial

-Occasional false positive

-Usally added to a programs build chain

 

#Auto type deduction

-Auto tells the compiler to deduce the type of a variable

-Aut0 forces initialization of variables

-Dont use aut0 to hide type information

 

#Range based for loops

-C++ will loop a range of values

-Can iterate over all STL Containers, strings and arrays

-You can add support of range-based for loops to your own types by supporting

The iterator interface

-Less reason to use bug prone for loop, with indexes

 

#Strongly typed enums

-Must use scope resolution operator

-Enums o longer exported to the surrounding scope

-Redefinition of types is no longer na issue

-Implicit converion is no longer allowed

-Prefer strongly typed enums to enums

 

#Using Lambdas

-A lambda expression is a unnamed function object

-Use a lambda to write a local function of limited use

-Use a lambda for complex initialization

-Prefer a lambda expression to std::bind

 

#Variadic Template Functions

-Types of each parameter can be different

-Must have a base case to accept one parameter

-Prefer variadic template functions to variadic functions

 

#How C++ and the STL Name Things

 

-Many programmers find C++ names confusing

-The short and sometimes cryptic names

-The use of snake casing

-C++ is old and some of the names even older

 

#Map, Filter and Reduce

-Many languages have map, filter and reduce methods

-C++ seems to lack them

#Be carefull with string_view

-String_view is a non-owning reference to a string

-It represents a view of a sequence of characters

-Be sure that the source lives longer than the string_view
#Amdahl's law

Overall Speedup = 1/((1-p)+(p/s))

p=portion of program that's parallelizable
s=Speedup of the parallelized portion 

Speedup = (Sequential execution time)/(Parallel execution time)

#Efficiency 

How well addition resources are utilized    

Efficiency = Speedup / # of processors 

#Design Parallel Programs
#Partitioning
#Parallel design stages
1.Partitioning: Break the problem into discrete pieces of work
2.Communication
3.Aglomeration 
4.Mapping

#Communication
Cordinate task execution and share information

#Synchronos blocking communication
-Task wait until entire communication is complete 
-Cannot do other work while in progress
#Asynchronous Nonblocking communication
-Task do not wait for communication to complete
-Can do other work while in progress

-Overhead: Compute time/resources spent on communication
-Latency: Time to send message from A to B
-Bandwidth: Amount to data communicated per seconds


#Aglomeration
-Combine task and replicate data/computation to increase efficiency

Granularity = Computation/Communication

-Fine-grained parallelism
1. large number of small task
2. Advantage: Good distribution of workload 
3. Disadvantage: low computation to communication ratio

-Coarse-grained parallelism
1. Small number os large task
2. Advantage: High computation-to-communication ratio
3. Disadvantage: Inefficient load balancing 

#Mapping
-Specify where each task will execute
-Does not apply to
1. Single core processors 
2. Automated task scheduling

#Goal
1. Minimize the total execution time 

#Strategies
1. Place task that can execute concurently on different processors 
2. Place task the communicate frequently on the same processor
 

